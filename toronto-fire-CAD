#!/bin/ksh93

# From http://www.toronto.ca/fire/cadinfo/livecad.htm :
#
#  The following active incidents are dispatched from Toronto Fire Services
#  Communication Centre. The contents are updated at five minute intervals
#  from the CAD (Computer Aided Dispatch) system.
#
# http://www.toronto.ca/fire/cadinfo/livecad.xml is the machine-readable
# version of the tabular information at the first link.  Apparently the
# update frequency is "every five minutes", so let's assume we're polling
# every 2.5 minutes (though of course this will be up to the cron job to
# actually implement).
#
# The result will be a directory of pairs of files (XML and headers)
# unique per retrieval but potentially duplicated in time.  This program
# should thus be complemented by a second half which iterates over that
# structure to pull out status updates and potentially populate a nicely
# queryable database.

typeset    dir_data dir_retr dir_dest url_xml fn_xml fn_hdr
typeset -i ret

dir_data=/home/gnomon/.toronto-fire-CAD
url_xml='http://www.toronto.ca/fire/cadinfo/livecad.xml'
fn_xml='livecad.xml'
fn_hdr='livecad.http'

printf '%(%s.%N)T.%d' now $$ | read dir_retr

dir_dest="${dir_data}/${dir_retr}"

mkdir -vp "${dir_data}/${dir_retr}" || exit 1

curl -v --trace-time -# \
  -D "${dir_dest}/${fn_hdr}" \
  -o "${dir_dest}/${fn_xml}" -- "$url_xml" || exit 2

# FIXME: this is where the iteration over the directory structure, the
# change detection (timestamp/ETag:/hash/content), and the database
# population will eventually live.
